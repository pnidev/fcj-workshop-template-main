[
{
	"uri": "https://thienluhoan.github.io/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Pham Thi Yen Nhi\nPhone Number: 0901143200\nEmail: xpnhi023@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 8/9/2025 to 9/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Get familiar with the AWS Console UI and core cloud concepts. Create and configure an AWS Free Tier account with MFA enabled for security. Learn basic cost control methods using AWS Budgets \u0026amp; Billing Alarms. Begin working with IAM for secure identity \u0026amp; permission management. Weekly Work Summary: Day Tasks Start Completed References 1 - Register AWS account - Enable MFA for root account - Configure initial cost alarm 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/vi/ 2 - Create AWS Budget with spending limits - Set email alert for budget thresholds - Explore Billing dashboard 10/09/2025 10/09/2025 https://000007.awsstudygroup.com/vi/ 3 - Review AWS Support plans + Basic / Developer / Business tiers - Compare response SLAs and suitable scenarios 11/09/2025 11/09/2025 https://000009.awsstudygroup.com/vi/1-support-plans/ 4 - Learn IAM Users \u0026amp; Groups - Read and interpret basic JSON policies 12/09/2025 12/09/2025 https://000002.awsstudygroup.com/vi/ 5 - Hands-on mini lab: + Enable MFA for IAM User + Test access control via policies + Create \u0026amp; delete S3 bucket for Free Tier test 13/09/2025 13/09/2025 https://000057.awsstudygroup.com/vi/ Week 1 Outcomes: Successfully created and secured AWS account with MFA protection. Learned how to apply spending limits using AWS Budgets and monitor usage. Understood the differences between AWS Support Plan tiers. Practiced IAM tasks including user creation, permission assignment, and policy testing. Performed basic S3 bucket actions and evaluated Free Tier usage impact. Summary: By the end of Week 1, I was able to:\nNavigate AWS Console confidently. Manage account cost with Budgets \u0026amp; usage alerts. Apply Free Tier cost control and track consumption trends. Configure IAM for secure, principle-based access control. Handle S3 storage operations and understand billing basics. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "This section presents my complete 12-week AWS learning worklog.\nThe roadmap was built progressively — starting with account setup and ending with designing production-ready cloud architectures with high resilience.\nLearning methodology\nI began the program on September 9, 2025, maintaining a consistent schedule of 5 study sessions per week.\nEach day, I spent around 2–3 hours practicing directly within AWS Console, reading official documentation, and completing labs from the First Cloud Journey (FCJ) curriculum.\nProgram Duration: 12 consecutive weeks — equivalent to one quarter of practical training.\nStudy breakdown:\n60% hands-on practice with AWS (deployment, configuration, debugging) 30% documentation review \u0026amp; architectural study 10% summarization, note-building \u0026amp; mini-challenge execution Progress Overview Over 12 weeks, I steadily built a strong cloud foundation, worked with over 25 AWS services, and completed a full deployment project at the final stage.\nBelow is the weekly breakdown with detailed worklogs:\nWeek 1: AWS Account Setup — Billing, Budgets, IAM\nWeek 2: Networking Basics — VPC, Subnets, Routes, Security \u0026amp; CLI\nWeek 3: Compute Services — EC2, IAM Roles, Cloud9\nWeek 4: Storage \u0026amp; Hosting — S3 Static Hosting, Lightsail, Containers\nWeek 5: Database Essentials — Amazon RDS \u0026amp; DynamoDB\nWeek 6: Performance Optimization — ElastiCache \u0026amp; 3-Tier Architecture\nWeek 7: Scalability \u0026amp; Monitoring — Auto Scaling \u0026amp; CloudWatch\nWeek 8: Global Delivery — Route 53, CloudFront, Lambda@Edge\nWeek 9: Windows Workloads — EC2 Windows \u0026amp; Managed AD\nWeek 10: Hybrid Identity — AD Connector \u0026amp; Enterprise Authentication\nWeek 11: High Availability — Multi-AZ, Load Balancer, DR\nWeek 12: Capstone — Full Deployment + Certification Prep\nMajor Takeaways Gained proficiency with 25+ AWS services across compute, network, storage, database \u0026amp; security Designed architectures with scalability, fault tolerance, and high availability Implemented cost optimization, logging, and resource monitoring strategies Worked with Windows Server workloads \u0026amp; Active Directory integration Completed a comprehensive capstone consolidating all learning outcomes Future Goals Prepare for the AWS Solutions Architect – Associate certification Explore serverless architecture further (Lambda, Step Functions, EventBridge) Engage with AWS communities, contribute to open-source and workshops "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway-lambda/5.4.1-rest-api/",
	"title": "API Gateway REST",
	"tags": [],
	"description": "",
	"content": "REST API Gateway We will create a REST API to connect the frontend to backend Lambda functions.\nRequired routes:\nPOST /score GET /leaderboard GET /leaderboard/global POST /progress POST /unlock POST /task/complete POST /money/add POST /shop/buy POST /avatar/presign POST /avatar/update POST /avatar/process ← calls the Lambda container for avatar processing Configuration:\nEnable CORS for all routes. Create a JWT Authorizer pointing to Cognito. Attach each route to the correct Lambda function. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nQuerying Amazon Aurora PostgreSQL using Amazon Bedrock Knowledge Bases with structured data Amazon Bedrock Knowledge Bases provides a fully managed Retrieval Augmented Generation (RAG) capability that allows Large Language Models (LLMs) to connect with internal data sources. This feature enhances the output of Foundation Models (FMs) by augmenting them with context from private datasets, making responses more accurate and relevant.\nAt AWS re:Invent 2024, AWS announced that Amazon Bedrock Knowledge Bases now supports natural language querying for retrieving structured data from Amazon Redshift and Amazon SageMaker Lakehouse. This capability enables a complete workflow for building Generative AI applications capable of accessing and integrating data from both structured and unstructured sources. Using Natural Language Processing (NLP), Amazon Bedrock Knowledge Bases can convert natural language queries into SQL statements, enabling users to retrieve data from supported sources without knowing database structure or SQL syntax.\nThis article demonstrates how to make data stored in Amazon Aurora PostgreSQL-Compatible Edition queryable using natural language through Amazon Bedrock Knowledge Bases, while still maintaining data freshness.\nStructured data retrieval in Amazon Bedrock Knowledge Bases and Amazon Redshift Zero-ETL Structured data retrieval in Amazon Bedrock Knowledge Bases allows natural language interaction with your database by converting user queries into SQL statements. When you connect a supported data source such as Amazon Redshift, Amazon Bedrock Knowledge Bases analyzes the database schema, table relationships, query engine, and historical queries to understand data context and structure. This knowledge enables the service to generate accurate SQL queries based on natural language questions.\nAt the time of writing, Amazon Bedrock Knowledge Bases supports structured data retrieval directly from Amazon Redshift and SageMaker Lakehouse. Although direct support for Aurora PostgreSQL-Compatible is not currently available, you can use zero-ETL integration between Aurora PostgreSQL-Compatible and Amazon Redshift to make your data accessible to Amazon Bedrock Knowledge Bases structured data retrieval. Zero-ETL integration automatically replicates Aurora PostgreSQL tables into Amazon Redshift near real-time, eliminating the need for complex ETL pipelines or data movement processes.\nThis architectural pattern is especially valuable for organizations that want to enable natural language querying over operational application data stored in Aurora PostgreSQL database tables. By combining zero-ETL integration with Amazon Bedrock Knowledge Bases, you can build powerful applications such as AI assistants powered by LLMs that deliver natural language responses based on operational data.\nSolution overview The following diagram illustrates the architecture we will deploy to connect Aurora PostgreSQL-Compatible with Amazon Bedrock Knowledge Bases using zero-ETL.\nWorkflow includes the following steps: Data is stored in Aurora PostgreSQL-Compatible inside a private subnet. A bastion host is used to securely connect to the database from a public subnet.\nUsing zero-ETL integration, this data is streamed into Amazon Redshift, also within a private subnet.\nAmazon Bedrock Knowledge Bases uses Amazon Redshift as the structured data source.\nUsers can interact with Amazon Bedrock Knowledge Bases through AWS Management Console or AWS SDK clients, submitting natural language queries. These queries are processed by Amazon Bedrock Knowledge Bases to retrieve information stored in Redshift (originating from Aurora).\nPrerequisites Ensure you are logged in with an IAM role that can: Create an Aurora database and run DDL (CREATE, ALTER, DROP, RENAME) and DML (SELECT, INSERT, UPDATE, DELETE) statements\nCreate a Redshift database\nConfigure zero-ETL integration\nCreate an Amazon Bedrock knowledge base\nSet up Aurora PostgreSQL database In this section, we walk through creating and configuring an Aurora PostgreSQL database with a sample schema for demonstration. We will create three related tables: products, customers, and orders.\nProvision the database In this section, we walk through creating and configuring an Aurora PostgreSQL database with a sample schema for demonstration. We will create three related tables: products, customers, and orders. Provision the database Start by provisioning a database environment. Create a new Aurora PostgreSQL database cluster and launch an EC2 instance to serve as a management entry point. The EC2 instance will help us create tables and maintain data throughout this guide. The following screenshot shows the details of the database cluster and EC2 instance. To follow the instructions for setup, reference Creating and connecting to an Aurora PostgreSQL DB cluster. Create the database schema Once connected to the database over SSH via your EC2 instance (as shown in Creating and connecting to an Aurora PostgreSQL DB cluster), it\u0026rsquo;s time to define the data structure. Use the following DDL statements to create three tables: \u0026ndash; Create Product table CREATE TABLE product ( product_id SERIAL PRIMARY KEY, product_name VARCHAR(100) NOT NULL, price DECIMAL(10, 2) NOT NULL );\n\u0026ndash; Create Customer table CREATE TABLE customer ( customer_id SERIAL PRIMARY KEY, customer_name VARCHAR(100) NOT NULL, pincode VARCHAR(10) NOT NULL );\n\u0026ndash; Create Orders table CREATE TABLE orders ( order_id SERIAL PRIMARY KEY, product_id INTEGER NOT NULL, customer_id INTEGER NOT NULL, FOREIGN KEY (product_id) REFERENCES product(product_id), FOREIGN KEY (customer_id) REFERENCES customer(customer_id) );\nPopulate data into the tables After tables are created, insert sample data. Ensure referential integrity is preserved when inserting into orders by confirming product_id exists in product and customer_id exists in customer.\nExample inserts:\nINSERT INTO product (product_id, product_name, price) VALUES (1, \u0026lsquo;Smartphone X\u0026rsquo;, 699.99); INSERT INTO product (product_id, product_name, price) VALUES (2, \u0026lsquo;Laptop Pro\u0026rsquo;, 1299.99); INSERT INTO product (product_id, product_name, price) VALUES (3, \u0026lsquo;Wireless Earbuds\u0026rsquo;, 129.99); INSERT INTO customer (customer_id, customer_name, pincode) VALUES (1, \u0026lsquo;John Doe\u0026rsquo;, \u0026lsquo;12345\u0026rsquo;); INSERT INTO customer (customer_id, customer_name, pincode) VALUES (2, \u0026lsquo;Jane Smith\u0026rsquo;, \u0026lsquo;23456\u0026rsquo;); INSERT INTO customer (customer_id, customer_name, pincode) VALUES (3, \u0026lsquo;Robert Johnson\u0026rsquo;, \u0026lsquo;34567\u0026rsquo;); INSERT INTO orders (order_id, product_id, customer_id) VALUES (1, 1, 1); INSERT INTO orders (order_id, product_id, customer_id) VALUES (2, 1, 2); INSERT INTO orders (order_id, product_id, customer_id) VALUES (3, 2, 3); INSERT INTO orders (order_id, product_id, customer_id) VALUES (4, 2, 1); INSERT INTO orders (order_id, product_id, customer_id) VALUES (5, 3, 2); INSERT INTO orders (order_id, product_id, customer_id) VALUES (6, 3, 3);\nBe sure to maintain referential integrity when inserting orders to avoid foreign key violations.\nYou may apply similar data samples to build schema and populate other tables.\nStaging ER7 microservice Lambda “trigger” registered with pub/sub hub, filtering messages by attribute Step Functions Express Workflow converts ER7 → JSON Two Lambdas: Format cleanup for ER7 (newline, carriage return) Parsing logic Results or errors are pushed back into pub/sub hub Configure Redshift cluster and zero-ETL After the Aurora database is prepared, proceed to configure zero-ETL integration with Amazon Redshift. This integration automatically synchronizes data from Aurora PostgreSQL-Compatible into Redshift.\nConfigure Amazon Redshift First, create a Redshift Serverless workgroup and namespace. Refer to Creating a data warehouse with Amazon Redshift Serverless.\nCreate zero-ETL integration Zero-ETL integration involves two main steps:\nCreate a zero-ETL integration from Aurora PostgreSQL to Redshift Serverless. After enabling integration on Aurora, create a corresponding database mapping on Redshift to ensure synchronized replication. The screenshot below illustrates our zero-ETL integration setup. Verification After configuration, verify successful integration.\nYou may confirm in the Redshift console by checking zero-ETL integration status. You should see Active along with source and destination as shown below.\nAdditionally, use Redshift Query Editor v2 to verify replicated data. A simple query like SELECT * FROM customer; should return the synchronized dataset from Aurora, as shown below: Configure Amazon Bedrock knowledge base with structured data The final step is to create an Amazon Bedrock knowledge base that supports structured querying.\nCreate Amazon Bedrock knowledge base Create a new Amazon Bedrock knowledge base with structured data selected. Refer to Build a knowledge base by connecting to a structured data store. After that, synchronize the query engine to grant data access.\nGrant data permissions Before synchronization succeeds, ensure appropriate permissions have been granted to the Amazon Bedrock Knowledge Base IAM role. This includes issuing GRANT SELECT commands for each table within Redshift.\nRun the following in Redshift Query Editor v2 per table: GRANT SELECT ON \u0026lt;table_name\u0026gt; TO \u0026ldquo;IAMR:\u0026rdquo;; Example: GRANT SELECT ON customer TO \u0026ldquo;IAMR:AmazonBedrockExecutionRoleForKnowledgeBase_ej0f0\u0026rdquo;;\nFor production environments, end-user identity must be federated for role-based data access. Reference AWS documentation for structured database access using identity federation. For web client identity federation, Amazon Cognito or SAML with AWS STS may be required based on your architecture.\nConfirm setup Once completed, your knowledge base should show: Status = Available\nQuery engine successfully synchronized with Amazon Redshift\nDatabase synchronization = COMPLETE\nYou may now query data using natural language.\nExample natural language queries Once Amazon Bedrock knowledge base is active, you may test structured data retrieval through natural language prompts. The service generates SQL automatically, runs the query, and uses LLMs to return readable responses.\nYou may test using:\n● Amazon Bedrock console → Knowledge Bases → Test tab\n● AWS SDK or API → Execute RetrieveAndGenerate programmatically\nConsole offers two modes of interaction:\n● Generate SQL queries – Shows raw SQL created from natural language input. ● Retrieval and response generation – Generates natural language output using an LLM. Table below shows example queries, SQL generated, and final responses. Natural Language Query Generate SQL Output Response Output Model How many customers do we have? SELECT COUNT(DISTINCT customer_id) AS num_customers FROM public.customer; We currently have 11 unique customers. Amazon Nova Lite Which customers have purchased the most products? SELECT c.\u0026ldquo;customer_name\u0026rdquo;, COUNT(o.\u0026ldquo;product_id\u0026rdquo;) AS \u0026ldquo;total_products_purchased\u0026rdquo; FROM public.customer c INNER JOIN public.orders o ON c.\u0026ldquo;customer_id\u0026rdquo; = o.\u0026ldquo;customer_id\u0026rdquo; GROUP BY c.\u0026ldquo;customer_name\u0026rdquo; ORDER BY \u0026ldquo;total_products_purchased\u0026rdquo; DESC; Based on the data, the customers who have purchased the most products are Charlie Davis, Alice Brown, and John Doe, each having purchased 14 products. Following closely are Jane Smith, Grace Lee, and Bob Johnson, who have each purchased 13 products. Henry Taylor, Frank Miller, and Eva Wilson have each purchased 12 products, while Ivy Anderson has purchased 11 products. Amazon Nova Lite Which customers have purchased more than one of the most expensive product? SELECT c.customer_name FROM public.customer c INNER JOIN public.orders o ON c.customer_id = o.customer_id INNER JOIN public.product p ON o.product_id = p.product_id WHERE p.price = (SELECT MAX(price) FROM public.product) GROUP BY c.customer_name HAVING COUNT(DISTINCT o.order_id); The customers who have purchased more than one number of the most expensive product are Grace Lee, Jane Smith, Alice Brown, and Eva Wilson. Amazon Nova Micro ● Retrieval and response generation – Generates natural language output using an LLM. Cleanup When finished, delete the resources created to avoid ongoing charges.\nConclusion This post demonstrated how to enable natural language querying on Aurora PostgreSQL data using Amazon Bedrock Knowledge Bases via zero-ETL integration with Amazon Redshift. We covered database setup, zero-ETL configuration, and knowledge base integration to retrieve data seamlessly. While this approach provides highly effective natural language access to structured data, consider Redshift storage cost when deploying in production.\nTry this solution and share your feedback in the comments section.\nAbout the authors Girish B is a Senior Solutions Architect at AWS India Pvt Ltd, based in Bengaluru. Girish works with various ISV customers to design and architect innovative solutions on AWS.\nDani Mitchell is a Generative AI Specialist Solutions Architect at AWS. She focuses on helping organizations worldwide accelerate their Generative AI journey with Amazon Bedrock.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nScaling Generative AI use cases – Part 1: Hub-and-spoke multi-tenant architecture using AWS Transit Gateway Generative AI continues to shape how enterprises approach innovation and problem solving. Customers are moving from the experimentation phase to scaling generative AI use cases across their organizations, with more and more businesses fully integrating these technologies into core processes. This evolution spans multiple lines of business (LOBs), teams, and Software-as-a-Service (SaaS) providers. Although many AWS customers typically start with a single AWS account to run generative AI proofs of concept, increased adoption and a shift to production environments have introduced new challenges.\nThese challenges include effectively managing and scaling deployments as well as abstracting and reusing cross-cutting concerns such as multi-tenancy, isolation, authentication, authorization, secure networking, rate limiting, and caching. To address these challenges efficiently, a multi-account architecture proves beneficial, particularly for SaaS providers serving many enterprise customers, large corporations with distinct business units, and organizations with strict compliance requirements. This multi-account approach helps you maintain a well-architected system by providing better organization, stronger security, and greater scalability in your AWS environment. It also enables you to manage these cross-cutting concerns more effectively as you scale your generative AI deployments.\nIn this two-part series, we discuss a hub-and-spoke architectural model for building a multi-tenant, multi-account architecture. This pattern supports abstraction of shared services across use cases and teams, helping you build secure, scalable, and reliable generative AI systems. In Part 1, we introduce a centralized hub for generative AI service abstractions and tenant-specific spokes, using AWS Transit Gateway to connect accounts. The hub account acts as the entry point for end-user requests, centralizing common capabilities such as authentication, authorization, model access, and routing decisions. This approach reduces the need to implement these capabilities separately in each spoke account. Where possible, we use VPC endpoints to access AWS services.\nIn Part 2, we discuss a variation of this architecture that uses AWS PrivateLink to securely share a centralized endpoint in the hub account with teams within the organization or with external partners.\nThe focus in both posts is on centralizing authentication, authorization, model access, and multi-account secure networking to onboard and scale generative AI use cases with Amazon Bedrock. We do not cover other system capabilities such as prompt catalog, prompt caching, versioning, model registry, and cost. However, those capabilities can be layered on as extensions to this architecture.\nSolution overview Our solution implements a hub-and-spoke pattern that provides a secure, scalable system for managing generative AI deployments across multiple accounts. At its core, the architecture consists of a centralized hub account that serves as the entry point for requests, complemented by spoke accounts that host tenant-specific resources. The following diagram illustrates this architecture: The hub account acts as the central account, providing shared services for multiple tenants and serving as the entry point for end-user requests. It centralizes common capabilities such as authentication, authorization, and routing decisions, eliminating the need to implement these functions separately for each tenant. The hub account is operated and maintained by the core engineering team.\nThe hub infrastructure includes public and private VPCs, an internet-facing Application Load Balancer (ALB), Amazon Cognito for authentication, and the necessary VPC endpoints for AWS services.\nThe spoke accounts host tenant-specific resources, such as AWS Identity and Access Management (IAM) role permissions and Amazon Bedrock resources. Spoke accounts may be managed by the core engineering team or by the tenants themselves, depending on organizational requirements.\nEach spoke account maintains its own private VPC, VPC interface endpoints for Amazon Bedrock, tenant-specific IAM roles and permissions, and other account-level controls. These components are connected via a Transit Gateway, which provides secure cross-account networking and manages traffic between the hub and spoke VPCs. The request flow through the system (as shown in the earlier architecture diagram) includes the following steps: A user (representing Tenant 1, 2, or N) accesses the client application.\nThe client application in the public subnet of the hub account authenticates the user and receives an ID/JWT token. In this example, we use an Amazon Cognito user pool as the identity provider (IdP).\nThe client application uses custom attributes in the JWT token to determine the appropriate route on the ALB. The ALB, based on the context path, routes the request to the tenant’s AWS Lambda function target group.\nThe tenant-specific Lambda function in the private subnet of the hub account is invoked.\nThis function assumes a cross-account role in the tenant’s account. The function calls Amazon Bedrock in the spoke account by referencing the regional DNS name of the Amazon Bedrock VPCE. The model is invoked and the result is returned to the user.\nThis architecture ensures that all requests pass through a central entry point while maintaining tenant isolation. By calling Amazon Bedrock in the spoke account, each request inherits that account’s limits, access controls, cost assignments, service control policies (SCPs), and other account-level controls.\nSample source code for this solution is split into two parts:\nThe first part demonstrates the solution for a single hub-and-spoke account.\nThe second part extends the solution by deploying an additional spoke account.\nDetailed, step-by-step instructions are provided in the repository README. In the following sections, we provide an outline of the deployment steps.\nPrerequisites We assume that you are familiar with the basics of AWS networking, including Amazon Virtual Private Cloud (Amazon VPC) and VPC constructs such as route tables and VPC interconnectivity options. We also assume that you understand multi-tenant architectures and the core principles of serving multiple tenants on shared infrastructure while maintaining isolation.\nTo deploy this solution, you need the following prerequisites: Hub and spoke accounts (required):\nTwo AWS accounts: one hub account and one spoke account\nAccess to the amazon.titan-text-lite-v1 model in the spoke account\nAdditional spoke account (optional):\nA third AWS account (a spoke account for a second tenant)\nAccess to the anthropic.claude-3-haiku-20240307-v1:0 model in the second spoke account\nDesign considerations Deploying this architecture involves several important design choices that affect how the solution operates, scales, and can be maintained. In this section, we examine these design considerations across the different components, explaining the rationale behind each choice and possible alternatives where appropriate.\nLambda functions In our design, the ALB target group is configured with Lambda functions running in the hub account instead of the spoke accounts. This approach enables centralized management of business logic, as well as centralized logging and monitoring. As the architecture evolves to include cross-cutting capabilities such as prompt caching, semantic routing, or the use of large language model (LLM) proxies (middleware services that provide unified access to multiple models while handling rate limiting and request routing, as discussed in Part 2), implementing these capabilities in the hub account ensures consistency across tenants.\nWe chose Lambda functions to implement token validation and routing logic, but you can also use other compute options such as Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), depending on your organization’s preferences.\nWe use a 1-to-1 mapping between Lambda functions and tenants. Although the current logic in each function is similar, having a dedicated function for each tenant can help reduce noisy neighbor issues and support tenant-tier-specific configurations such as memory and concurrency.\nVPC endpoints In this solution, we use dedicated Amazon Bedrock runtime VPC endpoints in the spoke accounts. Using dedicated VPC endpoints for each spoke account is suitable for organizations where operators of the spoke accounts are responsible for managing tenant features such as model access, knowledge bases, and guardrails.\nDepending on your organization’s policies, you may implement a different variant of this architecture by using centralized Amazon Bedrock runtime VPC endpoints in the hub account (as described in Part 2). Centralized VPC endpoints are suitable for organizations where a central engineering team manages features for tenants.\nOther factors such as cost, access control, and endpoint quotas must be considered when choosing between a centralized or dedicated approach for the placement of Amazon Bedrock VPC endpoints. With a centralized approach, VPC endpoint policies may hit the 20,480-character limit as the number of tenants increases. There are hourly fees for VPC endpoints and Transit Gateway attachments that are provisioned regardless of usage. If VPC endpoints are provisioned in spoke accounts, each tenant incurs additional hourly fees.\nClient application For demonstration purposes, the client application in this solution is deployed in the public subnet of the hub VPC. The application can be deployed in an account outside both the hub and spoke VPCs, or deployed at the edge as a single-page application (SPA) using Amazon CloudFront and Amazon Simple Storage Service (Amazon S3).\nTenancy Enterprises use different tenancy models when scaling generative AI, each with its own advantages and disadvantages. Our solution implements a silo model, assigning each tenant to a dedicated spoke account. For smaller organizations with fewer tenants and less stringent isolation requirements, an alternative approach is the pooled model (multiple tenants in a single spoke account), which may be more appropriate—unless they plan to scale significantly in the future or have specific compliance requirements. For more information on multi-tenancy design, see Let’s Architect! Designing architectures for multi-tenancy. Cell-based architectures for multi-tenant applications can provide benefits such as fault isolation and scaling. See Reducing the Scope of Impact with Cell-Based Architecture for more details.\nFrontend gateway In this solution, we choose ALB as the entry point for requests. ALB offers several advantages for our generative AI use case:\nLong-running connections – ALB supports connections up to 4,000 seconds, which is beneficial for LLM responses that may take more than 30 seconds to complete.\nScalability – ALB can handle a large volume of concurrent connections, making it suitable for enterprise-scale deployments.\nIntegration with AWS WAF – ALB integrates seamlessly with AWS WAF, providing enhanced security and protection against common web exploits.\nAmazon API Gateway is an alternative when you need API versioning, usage plans, or granular API management capabilities, and when message sizes and response times fit within its quotas. AWS AppSync is another option suitable when you want to expose LLMs through a GraphQL interface.\nChoose the gateway that best fits your customers:\nALB efficiently handles high-volume, long-running connections.\nAPI Gateway provides comprehensive REST API management.\nAWS AppSync delivers real-time GraphQL capabilities.\nEvaluate each option based on response time requirements, API needs, scale demands, and the specific use case of your application.\nAlthough this post demonstrates HTTP connectivity for simplicity, this is not recommended for production.\nProduction deployments must always use HTTPS with proper SSL/TLS certificates to maintain secure communication.\nIP addressing The AWS CloudFormation template for deploying solution resources uses example CIDRs. When deploying this architecture in a second spoke account, use unique IP address ranges that do not overlap with existing environments. Transit Gateway operates at Layer 3 and requires distinct IP spaces to route traffic between VPCs.\nDeploy a hub and spoke account In this section, we set up a local AWS Command Line Interface (AWS CLI) environment to deploy this solution in two AWS accounts. Detailed instructions are provided in the repository README. Deploy a CloudFormation stack in the hub account and another stack in the spoke account.\nConfigure connectivity between the hub and spoke VPCs using Transit Gateway attachments.\nCreate an Amazon Cognito user with the value tenant1 for a custom user attribute named tenant_id.\nCreate an item in the Amazon DynamoDB table to map the tenant ID to tenant-specific model access and routing information, in our example tenant1. Validate connectivity In this section, we validate connectivity from a test application in the hub account to an Amazon Bedrock model in the spoke account. We do this by sending a curl request from an EC2 instance (representing our client application) to the ALB. Both the EC2 instance and the ALB reside in the public subnet of the hub account. The request and response are then routed through Transit Gateway attachments between the hub and spoke VPCs. The following screenshot shows the execution of a utility script on your local workstation that authenticates a user and exports the necessary variables. These variables will be used to construct the curl request on the EC2 instance.\nThe next screenshot shows the curl request being executed from the EC2 instance to the ALB. The response confirms that the request was processed successfully and served by the amazon.titan-text-lite-v1 model, which is the model mapped to this user (tenant1). This model is hosted in the spoke account. Deploy a second spoke account In this section, we extend the deployment to include a second spoke account for an additional tenant.\nWe validate multi-tenant connectivity by sending another curl request from the same EC2 instance to the ALB in the hub account.\nDetailed instructions are provided in the repository README.\nThe following screenshot shows the response to this request, demonstrating that the system correctly identifies and routes requests based on tenant information.\nIn this case, the user’s tenant_id attribute value is tenant2, and the request is successfully routed to the anthropic.claude-3-haiku-20240307-v1:0 model, which is mapped to tenant2 in the second spoke account.\nClean up To clean up your resources, complete the following steps: If you created optional resources for the second spoke account, delete them: Change the directory to\ngenai-secure-patterns/hub-spoke-transit-gateway/scripts/optional\nRun the cleanup script\n./cleanupOptionalStack.sh\nClean up the main stack: Change the directory to\ngenai-secure-patterns/hub-spoke-transit-gateway/scripts/\nRun the cleanup script\n./cleanup.sh\nConclusion As organizations increasingly adopt and scale generative AI use cases across multiple teams and lines of business (LOBs), the need for secure, scalable, and reliable multi-tenant architectures continues to grow.\nThis two-part series addresses that need by providing guidance on how to implement the hub-and-spoke architecture pattern.\nBy adopting such well-architected practices early, you can build scalable and robust solutions that unlock the full potential of generative AI in your organization.\nIn this post, we showed how to set up a centralized hub account hosting shared services such as authentication, authorization, and networking using Transit Gateway.\nWe also demonstrated how to configure spoke accounts to host tenant-specific resources such as Amazon Bedrock.\nTry the provided code samples to see how this architecture works in practice.\nPart 2 will explore an alternative implementation using PrivateLink to interconnect VPCs in the hub and spoke accounts.\nAbout the Authors Nikhil Penmetsa is a Senior Solutions Architect at AWS. He helps organizations understand best practices related to advanced cloud-based solutions. He is passionate about diving deep with customers to build solutions that are cost-effective, secure, and performant. When he is not in the office, you can often find him putting in miles on his road bike or hitting the open road on his motorcycle.\nRam Vittal is a Principal ML Solutions Architect at AWS. He has over three decades of experience architecting and building distributed, hybrid, and cloud applications. He is passionate about building secure and scalable AI/ML and big data solutions to help enterprise customers in their cloud adoption and optimization journeys to improve their business outcomes. In his spare time, he rides his motorcycle and walks with his 3-year-old Sheepadoodle!\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nValidating event payloads with Powertools for AWS Lambda (TypeScript) In this post, learn how Powertools for AWS Lambda (TypeScript) with the new Parser utility can help you validate payloads easily and make your Lambda function more resilient.\nValidating input payloads is an important aspect of building secure and reliable applications. It ensures that data received by an application can smoothly handle unexpected or malicious inputs and prevent harmful downstream processing. When writing AWS Lambda functions, developers need to validate and verify the payload, while ensuring that specific fields and values are correct and safe to process.\nPowertools for AWS Lambda is a developer toolkit available for Python, NodeJS/TypeScript, Java, and .NET. It helps you implement serverless best practices and increase developer velocity. Powertools for AWS Lambda (TypeScript) now introduces a new Parser utility to make it easier for developers to implement validation in their Lambda functions.\nWhy payload validation matters Validating payloads can help your Lambda functions become more resilient. Payloads often combine both technical and business information, which can make them harder to validate. This requires you to write validation logic inside your Lambda function code. This can range from a few if-statements to check payload values to a chain of complex validation steps based on custom business logic. You may need to separate validation of technical information in the payload, such as AWS Region, accountId, event source, from business information in the event, such as productId and payment details.\nUnderstanding the structure and values of the event object, as well as how to extract the relevant information, can be challenging. For example, an Amazon SQS event has a body field with a string value that can be a JSON document. Amazon EventBridge has an object in the detail field that you can read directly without further transformation. You might need to decompress, decode, transform, and validate a payload inside a specific field. Understanding these multiple transformation layers can become complex, especially if your event object is the result of multiple service invocations.\nUsing Powertools for AWS Lambda (TypeScript) Parser Utility Powertools for AWS Lambda (TypeScript) is a modular library. You can selectively install features such as Logger, Tracer, Metrics, Batch Processing, Idempotency, and more. You can use Powertools for AWS Lambda in both TypeScript and JavaScript code bases. This new Parser utility simplifies validation and uses the popular validation library Zod.\nYou can use the parser as a method decorator, with middyjs middleware, or manually in all NodeJS runtimes provided by Lambda.\nTo use the utility, install the Powertools parser utility and Zod (\u0026lt;v3.x) using NPM or any package manager of your choice: npm install @aws-lambda-powertools/parser zod@~3 You can define your schema using Zod. Below is an example of a simple order schema to validate events: import { z } from \u0026lsquo;zod\u0026rsquo;; const orderSchema = z.object({ id: z.number().positive(), description: z.string(), items: z.array( z.object({ id: z.number().positive(), quantity: z.number(), description: z.string(), }) ), }); export { orderSchema }; The following schema defines id, description, and a list of items. You can specify value types ranging from simple numeric values, narrow them down to positive or literal values, or use more complex types such as unions, arrays, or even other schemas. Zod provides a rich set of value types that you can use.\nAdd the parser decorator to your handler function, set the schema parameter, and use this schema to parse the event object. import type {Context} from \u0026lsquo;aws-lambda\u0026rsquo;; import type {LambdaInterface} from \u0026lsquo;@aws-lambda-powertools/commons/types\u0026rsquo;; import {parser} from \u0026lsquo;@aws-lambda-powertools/parser\u0026rsquo;; import {z} from \u0026lsquo;zod\u0026rsquo;; import {Logger} from \u0026lsquo;@aws-lambda-powertools/logger\u0026rsquo;;\nconst logger = new Logger();\nconst orderSchema = z.object({ id: z.number().positive(),\ndescription: z.string(),\nitems: z.array( z.object({ id: z.number().positive(), quantity: z.number(), description: z.string(), }) ), });\ntype Order = z.infer;\nclass Lambda implements LambdaInterface { @parser({schema: orderSchema})\npublic async handler(event: Order, _context: Context): Promise { // event is now typed as Order for (const item of event.items) { logger.info(\u0026lsquo;Processing item\u0026rsquo;, {item}); // process order item from the event } } }\nconst myFunction = new Lambda();\nexport const handler = myFunction.handler.bind(myFunction); Note that z.infer helps extract the Order type from the schema, which improves the development experience with autocomplete when using TypeScript. Zod parses the entire object, including nested fields, and reports all combined errors instead of only returning the first error.\nUsing built-in schemas for AWS services A more common scenario is needing to validate events from AWS Services that trigger Lambda functions, including Amazon SQS, Amazon EventBridge, and many others. To make this easier, Powertools includes pre-built schemas for AWS events that you can use.\nTo parse an incoming Amazon EventBridge event, configure the built-in schema in your parser configuration:\nimport {LambdaInterface} from \u0026lsquo;@aws-lambda-powertools/commons/types\u0026rsquo;; import {Context} from \u0026lsquo;aws-lambda\u0026rsquo;; import {parser} from \u0026lsquo;@aws-lambda-powertools/parser\u0026rsquo;; import {EventBridgeSchema} from \u0026lsquo;@aws-lambda-powertools/parser/schemas\u0026rsquo;; import type {EventBridgeEvent} from \u0026lsquo;@aws-lambda-powertools/parser/types\u0026rsquo;;\nclass Lambda implements LambdaInterface {\n@parser({schema: EventBridgeSchema})\npublic async handler(event: EventBridgeEvent, _context: Context): Promise { // event is parsed but the detail field is not specified\n} }\nconst myFunction = new Lambda();\nexport const handler = myFunction.handler.bind(myFunction);\nThe event object is parsed and validated at runtime, and the EventBridgeEvent TypeScript type helps you understand the structure and access fields during development. In this example, you only parse the EventBridge event object, so the detail field can be an arbitrary object.\nYou can also extend the built-in EventBridge schema and override the detail field with your custom orderSchema:\nimport {LambdaInterface} from \u0026lsquo;@aws-lambda-powertools/commons/types\u0026rsquo;; import {Context} from \u0026lsquo;aws-lambda\u0026rsquo;; import {parser} from \u0026lsquo;@aws-lambda-powertools/parser\u0026rsquo;; import {EventBridgeSchema} from \u0026lsquo;@aws-lambda-powertools/parser/schemas\u0026rsquo;; import {z} from \u0026lsquo;zod\u0026rsquo;;\nconst orderSchema = z.object({\nid: z.number().positive(),\ndescription: z.string(),\nitems: z.array( z.object({ id: z.number().positive(), quantity: z.number(), description: z.string(), }), ), });\nconst eventBridgeOrderSchema = EventBridgeSchema.extend({ detail: orderSchema,});\ntype EventBridgeOrder = z.infer;\nclass Lambda implements LambdaInterface {\n@parser({schema: eventBridgeOrderSchema}) public async handler(event: EventBridgeOrder, _context: Context): Promise { // event.detail is now parsed as orderSchema\n} }\nconst myFunction = new Lambda();\nexport const handler = myFunction.handler.bind(myFunction);\nThe parser will validate the entire structure of the EventBridge event, including the custom business object.\nUse .extend or other Zod schema functions to modify any field of the built-in schema and customize payload validation.\nUsing envelopes with custom schemas In some cases, you only need the custom part of the payload, for example the detail field of an EventBridge event or the body of SQS records. This requires you to parse the event schema manually, extract the field you need, and then parse again using a custom schema. This is complex because you must know exactly which payload field to process and how to transform and parse it.\nThe Powertools Parser utility addresses this problem using Envelopes. Envelopes are schema objects with built-in logic to extract custom payloads.\nThe following is an example of EventBridgeEnvelope and how it works: import {LambdaInterface} from \u0026lsquo;@aws-lambda-powertools/commons/types\u0026rsquo;; import {Context} from \u0026lsquo;aws-lambda\u0026rsquo;; import {parser} from \u0026lsquo;@aws-lambda-powertools/parser\u0026rsquo;; import {EventBridgeEnvelope} from \u0026lsquo;@aws-lambda-powertools/parser/envelopes\u0026rsquo;; import {z} from \u0026lsquo;zod\u0026rsquo;;\nconst orderSchema = z.object({\nid: z.number().positive(), description: z.string(),\nitems: z.array( z.object({ id: z.number().positive(), quantity: z.number(), description: z.string(), }), ), });\ntype Order = z.infer;\nclass Lambda implements LambdaInterface {\n@parser({schema: orderSchema, envelope: EventBridgeEnvelope}) public async handler(event: Order, _context: Context): Promise { // event is now typed as Order inferred from the orderSchema\n} }\nconst myFunction = new Lambda();\nexport const handler = myFunction.handler.bind(myFunction);\nBy setting both the schema and the envelope, the parser utility knows how to combine both parameters, extract, and validate the custom payload from the event.\nPowertools Parser transforms the event object according to the schema definition, allowing you to focus on the business-critical portion inside your handler function.\nSafe parsing If the object does not match the provided Zod schema, by default the parser throws a ParserError. If you need to control validation errors and want to implement custom error handling, use the safeParse option.\nThe following is an example of how to capture failed validations as a metric in your handler function: import {Logger} from \u0026ldquo;@aws-lambda-powertools/logger\u0026rdquo;; import {LambdaInterface} from \u0026ldquo;@aws-lambda-powertools/commons/types\u0026rdquo;; import {parser} from \u0026ldquo;@aws-lambda-powertools/parser\u0026rdquo;; import {orderSchema} from \u0026ldquo;../app/schema\u0026rdquo;;import {z} from \u0026ldquo;zod\u0026rdquo;; import {EventBridgeEnvelope} from \u0026ldquo;@aws-lambda-powertools/parser/envelopes\u0026rdquo;; import {Metrics, MetricUnit} from \u0026ldquo;@aws-lambda-powertools/metrics\u0026rdquo;; import {ParsedResult, EventBridgeEvent} from \u0026ldquo;@aws-lambda-powertools/parser/types\u0026rdquo;;\nconst logger = new Logger();\nconst metrics = new Metrics();\ntype Order = z.infer;\nclass Lambda implements LambdaInterface {\n@metrics.logMetrics() @parser({schema: orderSchema, envelope: EventBridgeEnvelope, safeParse: true}) public async handler(event: ParsedResult\u0026lt;EventBridgeEvent, Order\u0026gt;, _context: unknown): Promise\u0026lt;void\u0026gt; { if (!event.success) { // failed validation metrics.addMetric('InvalidPayload', MetricUnit.Count, 1); logger.error('Invalid payload', event.originalEvent); } else { // successful validation for (const item of event.data.items) { logger.info('Processing item', item); // event.data is typed as Order } } } }\nconst myFunction = new Lambda();\nexport const handler = myFunction.handler.bind(myFunction);\nWhen safeParse = true, the parser does not throw an error. Instead, it returns a modified event object with a success flag and either error or data fields depending on the validation result.\nYou can implement custom error handling, for example incrementing an InvalidPayload metric and accessing originalEvent to log the error.\nFor successful validations, you can access the data field and process the payload.\nNote that the event object type is now ParsedResult, with EventBridgeEvent as the input and Order as the output type.\nCustom validations Sometimes you may need more complex business rules for validation.\nBecause Parser built-in schemas are Zod objects, you can customize validation by applying .extend, .refine, .transform, and other Zod operators.\nBelow is an example of complex rules for orderSchema: import {z} from \u0026lsquo;zod\u0026rsquo;;\nconst orderSchema = z.object({ id: z.number().positive(),\ndescription: z.string(),\nitems: z.array(z.object({ id: z.number().positive(), quantity: z.number(), description: z.string(),\n})).refine((items) =\u0026gt; items.length \u0026gt; 0, { message: \u0026lsquo;Order must have at least one item\u0026rsquo;,\n}), })\n.refine((order) =\u0026gt; order.id \u0026gt; 100 \u0026amp;\u0026amp; order.items.length \u0026gt; 100, { message: \u0026lsquo;All orders with more than 100 items must have an id greater than 100\u0026rsquo;, });\nUse .refine on the items field to check whether the order has at least one item. You can also combine multiple fields, such as order.id and order.items.length, to create specific rules for orders with more than 100 items. Note that .refine runs during the validation step, while .transform is applied after validation completes. This allows you to reshape the data to normalize the output.\nConclusion Powertools for AWS Lambda (TypeScript) introduces a new Parser utility that makes it easier to add validation to your Lambda functions.\nBy leveraging the popular validation library Zod, Powertools provides a rich set of built-in schemas for AWS service integrations such as Amazon SQS, Amazon DynamoDB, and Amazon EventBridge. Developers can use these schemas to validate event payloads and customize them to meet their business needs.\nVisit the documentation to learn more and join the Powertools community Discord to connect with fellow serverless enthusiasts.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-dynamodb-s3/5.3.1-dynamodb/",
	"title": "Create DynamoDB",
	"tags": [],
	"description": "",
	"content": "DynamoDB – Main Database In the game project, we use DynamoDB to store player profiles, progress, and scores.\nCreate 3 tables:\nUserProfiles Partition Key (PK): userId UserProgress Partition Key (PK): userId Scores Partition Key (PK): gameArea Sort Key (SK): score Enable DynamoDB Stream\nStream type: NEW IMAGE DevOps provides the Stream ARN to the backend so Lambda can consume events. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: AI/ML/GenAI on AWS (AWS Cloud Mastery Series #1) Event Objectives Introduced the overall AI/ML landscape in Vietnam and its potential for enterprise adoption Presented the complete Machine Learning lifecycle with Amazon SageMaker Demonstrated how to build and deploy Generative AI applications using Amazon Bedrock Equipped participants with prompt engineering techniques, RAG integration, Foundation Models, and chatbot deployment Speakers Key Highlights AI/ML Landscape \u0026amp; Community Connection Provided insights into AI/ML development trends in Vietnam Interactive ice-breaker session to connect attendees and speakers Encouraged knowledge exchange and networking among participants AWS AI/ML Services Amazon SageMaker – End-to-End ML Platform\nData processing, labeling, training, tuning, and deployment workflows MLOps integration for monitoring, scaling, and managing models efficiently Live demonstration of SageMaker Studio in practice Generative AI with Amazon Bedrock Overview of Foundation Models: Claude, Llama, Titan Prompt Engineering strategies: CoT reasoning, few-shot prompting, context enhancement RAG with Knowledge Bases for enhanced accuracy and domain context Bedrock Agents \u0026amp; Guardrails for safe, multi-step automation Live Demo: Building a functional Generative AI chatbot Event Experience The workshop offered a practical and structured understanding of how AI/ML and Generative AI can be deployed on AWS.\nThe balance between theoretical concepts and hands-on demonstrations made the learning experience clear and applicable.\nThe Bedrock section stood out the most, especially the live chatbot development which could be directly applied to real-world use cases.\nThis event strengthened my confidence in implementing GenAI solutions and gave me a clearer path to apply these technologies in future projects.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: ​DevOps on AWS (AWS Cloud Mastery Series #2) Event Objectives Gain a deeper understanding of DevOps culture and core principles Explore infrastructure as code and its real-world applications Improve knowledge of AWS container services and orchestration tools Strengthen skills in monitoring, logging, and system observability Speakers Truong Quang Trinh – Kỹ sư Nền tảng – AWS Community Builder tại TymeX Bao Huynh – AWS Community Builders Thinh Nguyen – AWS Community Builders Vi Tran – AWS Community Builders Long Huynh – AWS Community Builders Quy Pham – AWS Community Builders Nghiem Le – AWS Community Builders Key Highlights AWS CodeCommit Git-based source control service designed for secure, scalable code hosting Works well with GitFlow or Trunk-Based branching strategies Useful for team collaboration, review processes, and CI/CD automation AWS CodeBuild Fully managed build engine for compiling, testing, and packaging applications Customizable buildspec.yml to define build stages and environment steps Enables continuous integration with automatic test execution AWS CodeDeploy Deployment automation for EC2, Lambda functions, and on-premise systems Supports Blue/Green, Rolling Update, and Canary deployment patterns Helps minimize downtime while ensuring reliable releases AWS CodePipeline CI/CD pipeline service that orchestrates build → test → deploy steps Tracks version changes and automates multi-environment workflows Ideal for continuous delivery setups with minimal manual operations AWS CloudFormation Allows provisioning AWS infrastructure using declarative templates Supports stack updates, rollback protection, and drift detection Ensures consistent, repeatable infrastructure deployments AWS CDK (Cloud Development Kit) Write IaC using languages like TypeScript, Python, or JavaScript Uses high-level constructs to simplify infrastructure creation Compiles down to CloudFormation templates for production-ready deployment Container Services on AWS Discussed Docker containerization concepts for distributed workloads Amazon ECR: container registry with vulnerability scans and retention rules ECS \u0026amp; EKS: orchestration engines for scalable deployments AWS App Runner: simplest way to deploy container workloads without ops overhead Monitoring \u0026amp; Observability Amazon CloudWatch used for metrics collection, alarms, dashboards, log analysis AWS X-Ray for tracing requests and identifying performance bottlenecks Helps maintain application reliability and quick issue resolution DevOps Best Practices \u0026amp; Case Studies Introduced deployment models like feature toggles, canary rollout, A/B testing Encouraged automated testing inside CI/CD pipelines Covered real incident response workflows and lessons learned after failures Shared transformation journeys from startup to enterprise environments Event Experience Participating in DevOps on AWS helped me expand my understanding of DevOps workflows and AWS tooling. I gained clearer insight into how each service fits into automation, deployment, and monitoring — knowledge that I believe will support future project implementation and optimization. Some event photos "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event/",
	"title": "Event 7",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Well-Architected Security Pillar (AWS Cloud Mastery Series #3) Event Objective Gaining deeper understanding of AWS Identity services including IAM, IAM Identity Center (SSO), and permission management workflows. Learning logging, monitoring, and threat-detection tools such as CloudTrail, GuardDuty, Security Hub, and automation with EventBridge. Strengthening knowledge on Infrastructure Protection: VPC segmentation, secure network design, and public vs private resource placement. Studying cloud Data Protection principles and incident response best practices. Speakers Mendel Grabski (Long): Former Head of Security \u0026amp; DevOps, Cloud Security Solution Architect Tinh Truong: AWS Community Builder, Platform Engineer at TymerX Key Highlights Identity \u0026amp; Access Management Reviewed core IAM foundations: users, roles, and policies, emphasizing the removal of long-term access keys. Explored IAM Identity Center for unified access and permission control across accounts. Highlighted security essentials such as MFA enforcement, regular credential rotation, and identity validation using IAM Access Analyzer. Detection Learned how CloudTrail captures and audits all API activity within AWS environments. Explored threat-analysis tools including GuardDuty and Security Hub for consolidated security visibility. Understood multi-layer logging: VPC Flow Logs, ALB access logs, and S3 server-access logs. Designed alerting and automated response patterns using EventBridge workflows. Infrastructure Protection Covered AWS KMS fundamentals: key policies, grants, and rotation options. Reviewed encryption strategies for data at rest and in transit across key AWS services (S3, EBS, RDS, DynamoDB). Investigated secure secret-handling methods with Secrets Manager and Parameter Store, including rotation patterns. Emphasized data-classification principles and building guardrails for controlled access. Incident Response Discussed the AWS incident-response lifecycle and common remediation workflows. Examined scenarios: compromised IAM credentials, unintended S3 public access, EC2 malware detection. Practiced evidence-collection steps such as snapshots, workload isolation, and log retrieval. Learned how automation using Lambda and Step Functions can accelerate incident containment and recovery. Event Experience Joining the AWS Well-Architected Security Pillar workshop helped deepen my understanding of cloud security and gave me opportunities to meet students from various universities as well as professionals working with AWS. The hands-on insights into incident response and secure architecture broadened my perspective and enhanced my confidence when dealing with cloud-security challenges. I also had the chance to interact with international experts sharing practical experiences, motivating me to participate more in CloudClub activities for continuous learning and memorable experiences. Some event photos An opportunity to learn directly from experienced security practitioners.\nThis event expanded my understanding of AWS security, incident-handling strategies, and practical solutions. It inspired me to engage more with cloud-community activities to build both knowledge and experience.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Workshop Objective In this workshop, you will learn how to deploy a backend system for a game, including:\nCognito: login system with Username/Password and OAuth (Google). DynamoDB: storing player profiles, progress, and scores. S3: storing player avatars, only allowing uploads via pre-signed URL. Lambda: backend APIs using standard ZIP Lambdas and Container Lambda (OpenCV). API Gateway REST \u0026amp; WebSocket: serving REST routes and realtime leaderboard. CI/CD (CodePipeline + CodeBuild): automatically build container and deploy Lambda. IAM Roles: permissions for Lambda and CodeBuild. CloudWatch: basic logging, billing alarm, and error alarm. You will deploy each AWS service step by step, test using CLI or console, and finally collect all necessary information to hand over to FE and BE teams.\nLab Sections Cognito – Login System DynamoDB – Main Database S3 – Store Player Avatars ECR – Avatar Processing Container Lambda – Backend APIs API Gateway REST API Gateway WebSocket CI/CD – Backend IAM Roles Logging \u0026amp; Monitoring WAF (Optional) Prerequisites AWS account with sufficient IAM permissions to create and configure the services above. Use ap-southeast-2 region for this workshop. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Explore the fundamentals of AWS Networking through Amazon VPC. Understand core VPC components including Subnets, Route Tables, Internet Gateway, Security Groups, and NACLs. Practice internal network setup, routing, and internet connectivity through a networking workshop. Get familiar with AWS CLI and begin managing resources via command-line instead of Console UI. Weekly Work Summary: Day Tasks Start Completed Reference Docs 1 - Overview of Amazon VPC - Planning appropriate CIDR ranges - Create default VPC and verify access 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/vi/ 2 - Create Public + Private Subnets - Attach Route Tables - Connect Internet Gateway to public subnet 17/09/2025 17/09/2025 https://000004.awsstudygroup.com/vi/ 3 - Configure Security Groups - Study Network ACL behavior - Compare SG vs NACL in real scenarios 18/09/2025 18/09/2025 https://000003.awsstudygroup.com/vi/ 4 - Networking hands-on workshop - Configure NAT Gateway for private subnet - Experiment with VPC Peering 19/09/2025 19/09/2025 https://000003.awsstudygroup.com/vi/ 5 - Install AWS CLI - Run basic CLI commands Mini Lab: + Create VPC via CLI + List resources + Delete VPC using CLI 20/09/2025 20/09/2025 https://000011.awsstudygroup.com/vi/ Week 2 Outcomes: Developed a solid understanding of Amazon VPC and its networking components. Successfully configured Subnets, Route Tables, and Internet Gateway for connectivity. Clearly distinguished Security Groups (stateful) vs NACLs (stateless) and when to use each. Completed the hands-on lab involving VPC networking and connectivity setup. Installed and used AWS CLI for provisioning and deleting resources efficiently. Summary: By the end of Week 2, I accomplished:\nBuilt a custom VPC with both private and public subnets. Configured routing for public internet access and set up NAT for private subnets. Applied layered security using both SG and NACL. Managed AWS resources via CLI with improved speed and repeatability. Built a strong foundation for upcoming modules on EC2, Load Balancing, and Auto Scaling. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-dynamodb-s3/5.3.2-s3/",
	"title": "Create S3",
	"tags": [],
	"description": "",
	"content": "S3 – Store Player Avatars Create an S3 bucket:\nBucket name: game-avatars Enable CORS: allow PUT, POST, GET from all origins *. Note: Only allow uploads via presigned URLs generated by the backend.\nSummary You have created 3 DynamoDB tables and an S3 bucket. These resources will be used by the backend to store player data and avatars.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM Permissions Add the following IAM policy to your user account to deploy and manage the game backend project:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;GameProjectPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;codebuild:*\u0026#34;, \u0026#34;codepipeline:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;sns:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Serverless Multiplayer Game Backend A Scalable AWS Solution for Real-Time Gaming \u0026amp; AI Avatar Processing\n1. Executive Summary This proposal outlines the development of a fully serverless backend infrastructure for a multiplayer Unity game.\nThe system is divided into three main responsibilities: DevOps, Backend, and Unity Frontend.\nAWS services will be used to provide:\nUser Authentication → Amazon Cognito Gameplay \u0026amp; Business Logic → AWS Lambda Real-time Leaderboards → API Gateway WebSocket + DynamoDB Streams AI Avatar Generation → Lambda Container (OpenCV/MediaPipe) This architecture ensures scalability, fault tolerance, CI/CD automation, and supports seamless deployment for WebGL via itch.io or CloudFront.\n2. Problem Statement What’s the Problem? Multiplayer games require real-time backend capability, identity management, and persistent state storage.\nTraditional server-based systems:\nare expensive to run long-term scale poorly under heavy traffic spikes increase maintenance overhead Additionally, this project requires AI avatar transformation, which demands high compute power.\nThe Solution A scalable serverless architecture using AWS services:\nAuthentication: Amazon Cognito (User Pools + Hosted UI) Logic: AWS Lambda (Zip + ECR container images) Storage: S3 for avatars + DynamoDB for player metadata Realtime: WebSocket API + DynamoDB Streams Benefits \u0026amp; ROI Highly Cost Efficient → Pay-per-execution, no idle servers Auto Scaling → Handles traffic bursts without manual scaling CI/CD Ready → Quick feature rollout through CodePipeline/CodeBuild 3. Solution Architecture The system uses an event-driven microservice approach.\nUnity communicates with backend via REST APIs and WebSocket live streams, while avatar processing runs via container-based Lambda. AWS Services Used Category Services Identity Amazon Cognito API API Gateway REST + WebSocket Compute AWS Lambda (Zip + Container) Database DynamoDB + Streams Storage S3 Image Processing ECR Container Lambda CI/CD CodePipeline, CodeBuild Component Design Frontend Unity WebGL build deployed on itch.io / CloudFront.\nData Flow User Login → Cognito Issues Token Unity → REST API (Lambda → DynamoDB) Avatar Upload → Presigned URL → S3 AI Avatar Processor → Lambda Container (OpenCV/MediaPipe) Score Updates → DynamoDB Stream → WebSocket Broadcast Live 4. Technical Implementation Implementation Phases Infrastructure Setup (DevOps)\nCognito, DynamoDB, S3, API Gateway\nBackend Skeleton (BE)\nLambda endpoints + Postman scripts + API routing\nFrontend Login (FE)\nAuthManager + Cognito integration\nIntegration Wiring (DevOps)\nREST + WebSocket + DynamoDB Streams wiring\nGameplay API Integration (FE)\nDataManager for score, money, progress, tasks\nEnd-to-End Testing\nLogin → Game API → Leaderboard → Avatar\nDeployment\nUnity WebGL build + Allowed Redirect URLs\nTech Stack Requirements Role Stack Frontend Unity (C#), AwsConfig, AuthManager, DataManager, RealtimeManager Backend Lambda NodeJS/Python, Docker Container (OpenCV/MediaPipe) DevOps IAM, API Gateway, DynamoDB Streams, CodePipeline/Build 5. Timeline \u0026amp; Milestones Phase Duration Deliverables Foundation Days 1–3 Cognito, DynamoDB, S3, API Gateway Logic Development Days 3–8 Lambda API + Avatar Container + Unity Login Integration Days 8–12 Streams wired, APIs connected Testing \u0026amp; Launch Days 13–15 WebGL live deployment + Leaderboard \u0026amp; Avatar tests 6. Budget Estimation (Based on AWS Pricing Calculator)\nResource Cost Lambda Free Tier (low execution) DynamoDB Free Tier (25GB) S3 Storage $0.023/GB CloudWatch Logs ~$0.5 - $1/month ECR Storage ~$0.10/GB Estimated Total Cost \u0026lt; $5/month during development.\n7. Risk Assessment Risk Matrix Risk Impact Probability Integration complexity High Medium Latency during peak loads Medium Low Cost overrun Low Low Mitigation Strategy Mock APIs using Postman during FE development Placeholder logic for shop \u0026amp; leaderboard CloudWatch alarm triggers on error spikes (\u0026gt;10/min) 8. Expected Outcomes Technical Outcomes Fully serverless backend infrastructure Secure identity \u0026amp; data persistence Real-time leaderboard support Automated avatar processing pipeline Long-Term Value Reusable architecture for any future game titles Auto-scales without provisioning servers Extremely low operational cost "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway-lambda/5.4.2-websocket-api/",
	"title": "Websocket API",
	"tags": [],
	"description": "",
	"content": "WebSocket API Gateway WebSocket API is used for real-time leaderboard.\nRoutes:\n$connect → Lambda handler saves connectionId $disconnect → Lambda handler removes connectionId broadcast → Lambda handler sends leaderboard Configuration:\nLambda functions can read connectionIds from DynamoDB. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Gain a clear understanding of how Amazon EC2 works along with its instance families. Learn how to deploy, configure, and access EC2 instances inside a VPC. Explore IAM Roles for EC2 as a secure way to allow applications to access AWS services. Experience cloud-based development using AWS Cloud9 IDE. Weekly Work Summary: Day Tasks Start Completed References 1 - Introduction to EC2 - Learn instance classes based on CPU/RAM needs - Understand AMIs and their use cases 23/09/2025 23/09/2025 https://000003.awsstudygroup.com/vi/ 2 - Launch first EC2 instance - Configure Security Groups for SSH/HTTP access - Connect to server via SSH 24/09/2025 24/09/2025 https://000003.awsstudygroup.com/vi/ 3 - Create IAM Role with S3 permissions - Attach role to EC2 (no access keys needed) - Validate S3 access on server 25/09/2025 25/09/2025 https://000048.awsstudygroup.com/vi/ 4 - Set up workspace using AWS Cloud9 - Explore IDE interface, terminal, file management - Run small code snippets 26/09/2025 26/09/2025 https://000049.awsstudygroup.com/vi/ 5 - Mini Lab: + EC2 with IAM Role to access S3 + Deploy sample web application via Cloud9 + Practice stop/terminate for cost control 27/09/2025 27/09/2025 FCJ Internal Docs, AWS Blogs Week 3 Outcomes: Gained a strong foundation in EC2 architecture, AMI usage, and instance types. Successfully launched and accessed EC2 through SSH like a real remote server. Implemented IAM Role permissions for S3 without using static access keys. Became familiar with Cloud9 for code execution and basic deployments. Practiced full EC2 lifecycle management — run, configure, stop, terminate for cost optimization. Summary: By the end of Week 3, I was able to:\nDeploy EC2 instances securely inside a VPC using Security Groups. Assign IAM Roles to instances for safe AWS resource access. Connect to EC2 servers and manage applications directly via SSH. Use Cloud9 as a cloud-native IDE for development and testing. Understand EC2 costing and avoid charges by proper resource shutdown. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.3-dynamodb-s3/",
	"title": "About S3 &amp; DynamoDB",
	"tags": [],
	"description": "",
	"content": "Database \u0026amp; Storage Summary In the game project, we use AWS DynamoDB and AWS S3 to store player data, progress, scores, and avatars.\nDynamoDB – Main Database DynamoDB is used to store:\nPlayer profile information Player progress Player scores across different game areas Created Tables UserProfiles\nPartition Key (PK): userId UserProgress\nPartition Key (PK): userId Scores\nPartition Key (PK): gameArea Sort Key (SK): score DynamoDB Streams Enabled with Stream type: NEW IMAGE DevOps provides the Stream ARN to the backend so Lambda can consume events. S3 – Store Player Avatars S3 Bucket Setup Bucket name: game-avatars Enabled CORS: Allowed methods: PUT, POST, GET Allowed origin: * Note Uploads should only be performed using presigned URLs generated by the backend. Summary You have created:\n3 DynamoDB tables 1 S3 bucket for storing player avatars These resources will be used by the backend to securely manage and store player-related data.\nContent Create DynamoDB Create S3 Bucket "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway-lambda/5.4.3-lambda/",
	"title": "Lambda",
	"tags": [],
	"description": "",
	"content": "Lambda Functions Two types:\nStandard Lambda (ZIP) Used for: score, leaderboard, money, progress, unlock, task. Create one Lambda per API route. Lambda container Used for avatar AI (OpenCV + MediaPipe). Suggested name: AvatarProcessingLambda Creation: Lambda → Create Function → Container Image → choose image from ECR. DevOps only creates the function; backend implements the code. Summary You have created:\nREST API for the backend WebSocket API for real-time leaderboard Lambda functions (ZIP + Container) All these details are provided to the backend for connection and deployment.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog explains how to make data stored in Aurora PostgreSQL “queryable in natural language” using Amazon Bedrock Knowledge Bases. The authors use zero-ETL to automatically replicate data from Aurora to Amazon Redshift, then Bedrock Knowledge Bases converts English questions into SQL, runs them on Redshift, and returns human-readable answers. It walks through creating a sample schema (product, customer, orders), configuring zero-ETL, creating a knowledge base, and testing questions like “How many customers do we have?”.\nBlog 2 - \u0026hellip; This blog shows how to use Powertools for AWS Lambda (TypeScript) with the new Parser utility to validate event payloads (SQS, EventBridge, etc.) in a clean and maintainable way. The author uses the Zod library to define schemas (for example, an orderSchema), and the parser then parses and validates the event, can automatically extract the detail/body via “envelopes”, and supports safeParse so you can handle errors, logging, and metrics yourself. The post also demonstrates how to extend built-in schemas and add complex business rules using .refine().\nBlog 3 - \u0026hellip; This blog describes a multi-tenant, multi-account architecture for Generative AI use cases using Amazon Bedrock. The authors design a hub account as a shared entry point (ALB, Cognito, routing, authorization) and multiple spoke accounts dedicated to each tenant, all connected via AWS Transit Gateway. The hub receives requests, authenticates, routes them to a tenant-specific Lambda, which assumes a cross-account role and calls Bedrock in the tenant’s VPC, allowing centralized control (auth, routing) while preserving isolation, quotas, costs, and policies for each tenant.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Gain hands-on experience with Amazon S3 for web hosting and object storage. Explore Amazon Lightsail as a simplified cloud deployment option compared to EC2. Learn how to deploy applications using Lightsail Containers. Practice real application deployment workflows in a cloud production-style environment. Weekly Task Summary: Day Tasks Start Completed References 1 - Study Amazon S3 architecture - Learn bucket, object, storage class concepts - Configure static website hosting 30/09/2025 30/09/2025 https://000057.awsstudygroup.com/vi/ 2 - Static site hands-on: + Upload HTML/CSS/JS files + Enable public access + Test website via S3 URL 01/10/2025 01/10/2025 https://000057.awsstudygroup.com/vi/ 3 - Introduction to Amazon Lightsail - Compare features vs EC2 - Launch a test instance 02/10/2025 02/10/2025 https://000045.awsstudygroup.com/vi/ 4 - Explore Lightsail Containers - Learn basics of container image deployment - Deploy first containerized app 03/10/2025 03/10/2025 https://000046.awsstudygroup.com/vi/ 5 - Mini Project: + Fully deploy static website via S3 + Deploy a container app via Lightsail + Compare cost models: S3 vs Lightsail vs EC2 + Clean up unused resources 04/10/2025 04/10/2025 https://000057.awsstudygroup.com/vi/ Week 4 Outcomes: Successfully deployed a public static website using Amazon S3. Learned how to configure bucket permissions, access policies, and public access control. Understood how Lightsail provides simplified deployment compared to EC2. Deployed a container-based application using Lightsail Containers. Evaluated cost differences between S3, Lightsail, and EC2 for different project scopes. Summary: By the end of Week 4, I was able to:\nHost and manage static websites on S3 with custom permission policies. Deploy applications quickly using Lightsail without full EC2 configuration overhead. Build and run containerized workloads using Lightsail Containers. Choose the appropriate service (S3 / Lightsail / EC2) based on complexity, scale, and budget. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway-lambda/",
	"title": "API &amp; Lambda Overview",
	"tags": [],
	"description": "",
	"content": "API \u0026amp; Lambda Overview This section summarizes all API and compute services used in the game backend.\nThe system includes:\nREST API Gateway – Main API for gameplay actions WebSocket API Gateway – Real-time leaderboard communication AWS Lambda Functions – Backend compute for all game logic and avatar processing Below is a combined overview of how these services operate together.\nREST API Gateway – Backend API Layer The REST API connects the frontend (Unity/Web) to the backend Lambda functions.\nMain Endpoints POST /score GET /leaderboard GET /leaderboard/global POST /progress POST /unlock POST /task/complete POST /money/add POST /shop/buy POST /avatar/presign POST /avatar/update POST /avatar/process (proxy to the avatar container Lambda) Configuration Enable CORS for all routes Use JWT Authorizer connected to Amazon Cognito Each route is mapped to its own Lambda handler REST API ensures secure and structured communication for all core game features.\nWebSocket API – Real-Time Leaderboard The WebSocket API handles live leaderboard updates.\nRoutes $connect → store connectionId $disconnect → remove connectionId broadcast → push updated leaderboard to all players Configuration Lambda functions read active connectionIds from DynamoDB Ideal for timely score updates without polling This service allows the game to display real-time ranking updates.\nLambda Functions – Game Logic Execution Two Lambda types are used:\n1. Standard Lambda (ZIP) Used for standard game logic:\nscoring leaderboard player money progress saving unlocking features task system Each API route has a dedicated Lambda function to keep logic clean and modular.\n2. Container Lambda Used for heavy AI processing:\nAvatar generation (OpenCV + MediaPipe) Function name suggestion: AvatarProcessingLambda Deployed from an ECR container image.\nDevOps prepares the function, and backend developers implement the code.\nSummary You have successfully set up:\nA REST API for game logic communication A WebSocket API for real-time leaderboard Multiple Lambda functions (ZIP + Container) for all backend processing These services form the core backend infrastructure for the game, enabling secure API access, real-time updates, and AI-powered avatar processing.\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Well-Architected Security Pillar (AWS Cloud Mastery Series #3)\nDate \u0026amp; Time: 08:30, December 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: ​DevOps on AWS (AWS Cloud Mastery Series #2)\nDate \u0026amp; Time: 08:30, December 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AI/ML/GenAI on AWS (AWS Cloud Mastery Series #1)\nDate \u0026amp; Time: 08:00, December 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand how Amazon RDS operates and when relational databases are the right choice. Deploy, configure, and test-connect to RDS instances inside a VPC environment. Explore Amazon DynamoDB — a fully managed NoSQL service built for high performance and scalability. Compare RDS vs DynamoDB to identify which database model suits different workloads. Weekly Tasks Completed: Day Task Start Completed References 1 - Overview of Amazon RDS - Learn MySQL/PostgreSQL engines - Study Multi-AZ replication \u0026amp; Read Replica behavior 07/10/2025 07/10/2025 https://000005.awsstudygroup.com/vi/ 2 - Create first RDS instance - Configure Security Groups for DB access - Connect through EC2 + MySQL Workbench/CLI 08/10/2025 08/10/2025 https://000005.awsstudygroup.com/vi/4-create-rds/ 3 - Learn snapshots \u0026amp; automated backups - Restore DB from snapshot - Test data recovery scenarios 09/10/2025 09/10/2025 https://000005.awsstudygroup.com/vi/6-backup/ 4 - Introduction to DynamoDB - Understand NoSQL model, PK/SK schema design - Create table and run queries 10/10/2025 10/10/2025 https://000060.awsstudygroup.com/vi/ 5 - Mini Lab: + Create MySQL RDS DB + CRUD operations + Create DynamoDB table \u0026amp; read/write data + Benchmark RDS vs DynamoDB performance + Resource cleanup 11/10/2025 11/10/2025 https://000005.awsstudygroup.com/vi/5-deploy-app/ Week 5 Achievements: Gained solid understanding of RDS operation, replication, and backup mechanisms. Successfully created, configured, and queried RDS from EC2. Performed DB restoration using automated snapshots. Created DynamoDB tables and executed CRUD operations via Console/SDK. Built practical comparison insight between relational vs NoSQL performance. Summary: By the end of Week 5, I achieved the following:\nFully operated an RDS MySQL instance — from creation to backup \u0026amp; recovery. Implemented DB access control using Security Groups safely. Understood snapshot/restore for failure recovery and rollback scenarios. Designed DynamoDB schema for high-traffic workloads. Identified use cases where RDS is stronger for transactions, while DynamoDB excels in large-scale, low-latency distributed applications. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.5-ci-cd/",
	"title": "CI/CD Deployment for Backend",
	"tags": [],
	"description": "",
	"content": "Objective In this section, you will configure an automated pipeline to deploy the backend, including both regular Lambda and Lambda container (AvatarProcessingLambda). CI/CD ensures correct IAM permissions and high availability of services.\n1. Prerequisites Backend GitHub repository ECR repository for Lambda container (AvatarProcessingLambda) IAM Role for CodeBuild with permissions: ECR push/pull, Lambda update 2. Create CodePipeline Go to AWS CodePipeline → Create Pipeline Name the pipeline (e.g., GameBackendPipeline) Select Source: GitHub repo Select Build: CodeBuild 3. Configure CodeBuild Environment: Managed image + Docker Sample buildspec.yml: version: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... - aws ecr get-login-password --region ap-southeast-2 | docker login --username AWS --password-stdin 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com build: commands: - echo Build Docker image... - docker build -t avatar-processing . - docker tag avatar-processing:latest 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com/avatar-processing:latest post_build: commands: - echo Push Docker image to ECR... - docker push 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com/avatar-processing:latest - echo Deploy to Lambda... - aws lambda update-function-code --function-name AvatarProcessingLambda --image-uri 254670366571.dkr.ecr.ap-southeast-2.amazonaws.com/avatar-processing:latest "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Workshop Overview — Game Backend Project General Objectives This workshop walks you step-by-step through deploying a complete backend for a Unity/Web game on AWS. After completing it, you will have:\nUser authentication via Cognito (username/password + Google OAuth). A NoSQL database using DynamoDB for profiles, progress, and scores. Player avatar storage on S3 with presigned URL uploads. Lambda functions (ZIP) for lightweight logic and a Container Lambda (OpenCV/MediaPipe) for heavy image/AI processing. API Gateway (REST) for gameplay routes and API Gateway (WebSocket) for real‑time leaderboard. CI/CD pipeline (CodePipeline + CodeBuild) for automatic container build, ECR deployment, and Lambda updates. IAM configuration, CloudWatch monitoring/logging, and optional WAF. Workshop Structure Cognito — Set up authentication + JWT authorizer. DynamoDB — Create tables: UserProfiles, UserProgress, Scores. S3 — Create game-avatars bucket, enable CORS, use presigned URLs. ECR — Create repository for Container Lambda images. Lambda — Implement ZIP functions + container-based Lambda for image processing. API Gateway (REST) — Define routes and integrate with Lambda. API Gateway (WebSocket) — Manage realtime leaderboard communication. CI/CD — Set up CodePipeline + CodeBuild to automate builds and deployments. IAM — Create proper roles for Lambda, CodeBuild, and CodePipeline. Logging \u0026amp; Monitoring — Configure CloudWatch logs and alerts. WAF — (Optional) Protect API from malicious requests. Technical Details DynamoDB Main tables: UserProfiles (PK: userId) UserProgress (PK: userId) Scores (PK: gameArea, SK: score) Streams enabled with NEW_IMAGE for realtime event processing. S3 Bucket: game-avatars CORS: allow PUT, POST, GET (production should be more restrictive). Uploads handled via presigned URLs generated by backend. API \u0026amp; Lambda Common REST endpoints: POST /score, GET /leaderboard, GET /leaderboard/global POST /progress, POST /unlock, POST /task/complete POST /money/add, POST /shop/buy POST /avatar/presign, POST /avatar/update, POST /avatar/process WebSocket routes: $connect, $disconnect, broadcast Lambda Types: ZIP Lambdas for scoring, shop, progress, leaderboard. Container Lambda AvatarProcessingLambda using OpenCV + MediaPipe. CI/CD (CodePipeline + CodeBuild) Source: GitHub repository Build: CodeBuild (Docker enabled) → build image → push to ECR → update Lambda image. Provided buildspec.yml logs in to ECR, builds the image, pushes it, and updates Lambda. IAM Required permissions include Lambda, API Gateway, Cognito, DynamoDB, S3, CodeBuild, CodePipeline, CloudWatch, SNS, and iam:PassRole. Monitoring CloudWatch: log groups for Lambda, alarms for billing and errors. Workshop Outcomes Full backend system for a game: REST API, realtime WebSocket, database, avatar storage, and automated deployment pipeline. Configuration notes (ARNs, endpoints, presigned URL formats) for frontend/backend developers. Content Workshop overview Prerequiste Creating DynamoDB and S3 Bucket Creating API Gateway \u0026amp; Lambda More about CI/CD Clean up "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Gain familiarity with Amazon ElastiCache and understand cache usage in distributed systems. Learn how Redis/Memcached improves query performance through caching. Integrate ElastiCache with services used in previous weeks for real-world application flow. Review RDS + DynamoDB knowledge to build a performance-focused architecture using cache. Weekly Tasks Completed: Day Task Start Completed References 1 - Overview of ElastiCache - Difference between Redis vs Memcached - Learn cache-first \u0026amp; cache-aside patterns 14/10/2025 14/10/2025 https://000061.awsstudygroup.com/vi/1-introduce/ 2 - Create Redis Cluster in ElastiCache - Configure VPC Security Groups - Test connection from EC2 15/10/2025 15/10/2025 https://000061.awsstudygroup.com/vi/3-amazonelasticacheforredis/3.4-grantaccesstocluster/ 3 - Implement caching layer for application - Apply TTL \u0026amp; manual invalidation - Monitor cache hit/miss ratio 16/10/2025 16/10/2025 AWS Caching Guidelines, FCJ Notes 4 - Integration Lab: + Connect RDS + Redis + Store sessions \u0026amp; cache query results + Measure response performance 17/10/2025 17/10/2025 AWS Architecture Labs 5 - Mini Project: + Deploy full 3-tier architecture (EC2 → Redis → RDS) + Benchmark with \u0026amp; without cache + Optimize DB architecture \u0026amp; cost + Clean up resources to avoid charges 18/10/2025 18/10/2025 Well-Architected Framework, FCJ Session Week 6 Achievements: Understood the role of In-Memory Cache and why Redis is essential in large-scale systems. Clearly differentiated Redis (session/state caching) vs Memcached (simple key-value memory store). Successfully built a Caching Layer to reduce DB query load. Integrated ElastiCache + RDS to significantly improve query response speed. Completed a scalable web architecture demo with improved performance. Summary: By the end of Week 6, I was able to:\nCreate and connect to an ElastiCache Redis cluster in a private VPC. Design cache logic for frequently accessed data and API responses. Integrate Redis to store session/state values and cache database queries. Benchmark performance gains and evaluate cache benefit on system load. Understand how RDS + DynamoDB + Redis complement each other in real-world system design. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "5.6 Cleaning up the Workshop Environment After completing the workshop, clean up the AWS environment to avoid unnecessary charges.\nDelete Lambda Functions Go to AWS Lambda Console. Select all Lambda Functions created for the workshop: AvatarProcessingLambda BroadcastHandler ConnectHandler DisconnectHandler LeaderboardFunction MoneyFunction ProgressFunction ScoreFunction TaskFunction UnlockFunction Click Actions → Delete and confirm. Delete API Gateway Go to API Gateway Console. Select REST API and WebSocket API created for the workshop. Click Actions → Delete API and confirm. Delete S3 Buckets Go to S3 Console. Delete all buckets created: game-avatars Workshop-specific buckets (bucket-1, bucket-2, …) Make sure to delete all objects inside each bucket before deleting the bucket. Delete DynamoDB Tables Go to DynamoDB Console. Delete the following tables: UserProfiles UserProgress Scores Confirm deletion of each table. Delete IAM Roles Go to IAM Console → Roles. Delete the roles created for the workshop: Lambda Execution Role CodeBuild Role Ensure no policies are attached before deleting roles. Delete ECR Repositories Go to ECR Console. Delete the avatar-processing repository. Confirm deletion of all images in the repository. Delete CodePipeline / CodeBuild Go to CodePipeline Console. Delete the workshop pipeline. Delete the corresponding CodeBuild project. Check Costs Go to AWS Billing Console → Cost Explorer. Make sure there are no running resources incurring costs. Delete any remaining resources if necessary. Summary The workshop environment has been fully cleaned. No Lambda, API Gateway, S3, DynamoDB, IAM Roles, ECR, or pipelines remain. Costs from the workshop should now be zero. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at FCJ AWS from September 8, 2025 to December 9, 2025, I had the opportunity to work directly in a real-world professional environment where I could apply academic knowledge while learning new workflows used in the organization.\nMy main responsibility was developing a 2D endless runner game integrated with AWS services, which helped me improve significantly in programming, tool usage, teamwork, and task reporting.\nBesides technical skills, I developed soft skills such as time management, communication, problem-solving and the ability to collaborate with teammates to complete sprint goals. I was proactive in researching, asking questions when needed, and following internal work processes.\nTo objectively reflect on the entire internship journey, I evaluated myself based on the following criteria:\nNo. Evaluation Criteria Description Good Fair Average 1 Professional Knowledge Understanding processes, tools, and applying skills effectively ✅ ☐ ☐ 2 Ability to Learn Acquiring new technologies and self-studying efficiently ☐ ✅ ☐ 3 Proactiveness Taking initiative without waiting for instruction ✅ ☐ ☐ 4 Responsibility Completing tasks on time with minimal errors ✅ ☐ ☐ 5 Discipline Sometimes late on small deadlines — needs improvement ☐ ☐ ✅ 6 Growth Mindset Open to feedback, eager to learn and develop ☐ ✅ ☐ 7 Communication \u0026amp; Reporting Clear communication but needs more confidence when presenting ☐ ✅ ☐ 8 Teamwork Actively supports teammates and collaborates well ✅ ☐ ☐ 9 Professional Attitude Respectful, collaborative, and adaptable ✅ ☐ ☐ 10 Problem Solving Can analyze issues but solution optimization needs improvement ☐ ✅ ☐ 11 Contribution to Project Engaged, responsible, and positively impacts project progress ✅ ☐ ☐ 12 Overall Performance Shows consistent growth and positive development ✅ ☐ ☐ Needs Improvement Strengthen discipline to avoid minor deadline slips Improve solution-oriented thinking and analytical skills Build communication confidence in meetings and presentations "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Explore how EC2 Auto Scaling maintains application performance during traffic spikes or drop-offs. Create and configure Auto Scaling Groups using custom Launch Templates. Get hands-on experience with Amazon CloudWatch for real-time resource monitoring. Implement automated scaling policies based on system metrics. Weekly Tasks Completed: Day Task Start Completed Reference Docs 1 - Study the core concept of Auto Scaling - Compare scaling policies: Target Tracking / Step / Scheduled - Review Launch Template fundamentals 21/10/2025 21/10/2025 https://000006.awsstudygroup.com/vi/ 2 - Create custom Launch Template - Deploy Auto Scaling Group in VPC - Configure Min–Desired–Max capacity parameters 22/10/2025 22/10/2025 https://000006.awsstudygroup.com/vi/6-create-auto-scaling-group/ 3 - Learn CloudWatch Metrics \u0026amp; Namespaces - Build monitoring Dashboard for EC2 activities 23/10/2025 23/10/2025 https://000008.awsstudygroup.com/vi/ 4 - Create CloudWatch Alarm based on CPU threshold - Enable SNS email notifications - Bind alarm to scaling policies 24/10/2025 24/10/2025 https://000008.awsstudygroup.com/vi/5-cloud-watch-alarm/ 5 - Hands-on Lab: + Deploy Auto Scaling Group + Scaling Policies + Load test to trigger scaling events + Observe scaling logs in CloudWatch + Release resources after test completion 25/10/2025 25/10/2025 FCJ Training Notes, Monitoring Best Practices Week 7 Achievements: Gained a deeper understanding of how EC2 Auto Scaling functions under variable workload. Successfully created and configured Launch Template + Auto Scaling Group for dynamic scaling. Built CloudWatch dashboards to track CPU, Network metrics, and system health checks. Setup automatic alerting using CloudWatch Alarm + SNS Notification. Simulated load and observed scale-out/scale-in behavior in real-time. Summary: By the end of Week 7, I was able to:\nDeploy an Auto Scaling environment that maintains stable application performance. Select suitable scaling policies based on workload patterns and thresholds. Monitor instance metrics using CloudWatch dashboards and respond via alert triggers. Conduct load testing to validate automatic scaling behavior. Optimize EC2 resource usage to balance uptime reliability and cost efficiency. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment The working environment at FCJ is comfortable and easy to adapt to. Members within the FCJ community are friendly, supportive, and willing to help even outside working hours. The workspace is clean and well-organized, which helps me stay focused and productive.\n2. Support from Mentor / Team Admin My mentor provided clear guidance and thoroughly explained concepts when I encountered difficulties, while still encouraging me to explore and solve problems independently. The mentor team also assisted with procedures, shared resources and documentation, and ensured I had the tools required for my tasks. I truly appreciate their patience and the approach of helping me grow instead of giving direct solutions.\n3. Relevance of Work to Academic Major The tasks assigned were closely related to the knowledge I learned at university, especially in software development workflows and data handling. Moreover, I was exposed to new technologies, helping me strengthen my fundamentals and expand my practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nThroughout the internship, I enhanced my ability to manage tasks, use project support tools, and collaborate effectively in a team. My mentor also shared many real-world scenarios, helping me build a clearer vision for my future career path.\n5. Company Culture \u0026amp; Team Spirit The team works professionally while maintaining a welcoming and supportive atmosphere. Everyone respects one another’s ideas and steps in when help is needed. During high workload periods, the team works together instead of dividing pressure individually. This made me feel truly included, even as an intern.\n6. Internship Policies / Benefits\nThe internship provided a supportive learning setup with good facilities, essential tools, and helpful mentor guidance, enabling me to grow effectively throughout the program. Additional Questions What satisfied me most during this internship was gaining exposure to real development workflows and receiving dedicated mentorship. FCJ could consider expanding knowledge-sharing sessions across teams to increase interaction and mutual learning. I would recommend this internship to friends, as it offers a friendly environment and strong growth opportunities. Suggestions \u0026amp; Expectations I hope to participate in more internal workshops on new technologies to broaden technical understanding. I am willing to continue with the program in the future if given the chance. This internship has helped me grow in both mindset and skillset — I sincerely appreciate the support and experience FCJ has provided. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Learn how Amazon Route 53 handles DNS management in cloud environments. Explore Amazon CloudFront — a CDN service for global content acceleration. Understand the role of Lambda@Edge in processing logic at edge locations. Build a low-latency global content delivery architecture with reliability and scalability. Weekly Tasks Completed: Day Task Start Completed Reference Docs 1 - Overview of Route 53 - Review DNS concepts: A, CNAME, Alias - Create hosted zone and basic records 28/10/2025 28/10/2025 https://000010.awsstudygroup.com/vi/ 2 - Apply routing policies: Simple, Weighted, Failover - Configure Health Check for failover automation 29/10/2025 29/10/2025 https://000010.awsstudygroup.com/vi/2-prerequiste/ 3 - Introduction to CloudFront CDN - Study edge locations and cache TTL concepts - Create first distribution 30/10/2025 30/10/2025 https://000094.awsstudygroup.com/vi/ 4 - Connect CloudFront with S3 origin - Map custom domain using Route 53 - Generate SSL certificate via ACM 31/10/2025 31/10/2025 https://000094.awsstudygroup.com/vi/1.-cloud-front-với-s3/ 5 - Learn how Lambda@Edge works - Deploy CloudFront + Custom Domain + SSL - Implement request/response logic at edge - Monitor CDN performance and latency 01/11/2025 01/11/2025 https://000130.awsstudygroup.com/vi/ Week 8 Achievements: Gained a solid understanding of DNS management using Route 53. Successfully applied routing policies for failover and traffic distribution. Built a functional CloudFront CDN to deliver content globally. Used Lambda@Edge to process logic directly at edge networks. Integrated Route 53 + CloudFront + ACM SSL + Edge computing into a full CDN pipeline. Summary: By the end of Week 8, I was able to:\nOptimize global content delivery using Route 53 and CloudFront. Issue SSL/TLS certificates via ACM and apply them to custom domains. Customize CloudFront behavior using Lambda@Edge request/response logic. Reduce latency for users across multiple regions with a global CDN approach. Understand how to scale applications to global-level performance. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Explore how to operate Windows Workloads on AWS. Launch and administer Windows Server hosted on EC2. Get familiar with AWS Managed Microsoft AD for centralized identity management. Experiment with integrating Windows workloads into various AWS enterprise services. Weekly Task Implementation: Day Task Start Completed Reference Materials 1 - Review foundational concepts of Windows on AWS - Understand Windows EC2 licensing model - Launch first Windows Server EC2 instance 04/11/2025 04/11/2025 https://000093.awsstudygroup.com/vi/ 2 - Connect to EC2 Windows using Remote Desktop (RDP) - Enable necessary Windows roles and features - Deploy IIS Web Server on Windows EC2 05/11/2025 05/11/2025 FCJ Notes, AWS Windows Hands-on 3 - Study AWS Managed Microsoft AD - Understand AD structure: Domain, OU, Group - Deploy Directory Service on AWS 06/11/2025 06/11/2025 AWS Directory Service Docs 4 - Join Windows EC2 to Directory Domain - Create user, group, and assign permissions - Configure Group Policies for centralized management 07/11/2025 07/11/2025 AWS Managed AD Whitepaper 5 - Final hands-on practice: + Deploy Windows application on EC2 + Integrate authentication with Managed AD + Test domain login + SSO + Build and document user management workflow 08/11/2025 08/11/2025 AWS Enterprise Patterns, FCJ Workshop Achievements: Successfully launched and managed Windows Server on EC2. Connected via RDP, installed IIS, and hosted a sample web application. Deployed and configured AWS Managed Microsoft AD for identity services. Joined Windows EC2 to domain and centrally managed users and groups. Practiced policy administration using Group Policy and SSO authentication. Summary: By the end of Week 9, I was able to:\nIndependently deploy and operate Windows workloads on AWS. Install IIS and host applications directly on EC2 Windows instances. Build Active Directory using Managed AD and apply it to real scenarios. Join servers to a domain and manage authentication and user access. Operate Windows-based environments on AWS using enterprise-grade approaches. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Gain solid understanding of Hybrid Active Directory integration. Implement connectivity between AWS Managed Microsoft AD and on-premises AD. Learn when and how to use AD Connector \u0026amp; Simple AD in real environments. Practice monitoring, troubleshooting, and optimizing directory operations. Workload for the Week: Day Task Start Done Reference 1 - Review Hybrid Active Directory architecture - Understand domain trust relationships - Build a basic trust model 11/11/2025 11/11/2025 AWS Hybrid AD Docs 2 - Research AD Connector - Compare Managed AD vs AD Connector vs Simple AD - Test connectivity with on-prem AD 12/11/2025 12/11/2025 AWS Directory Service Overview 3 - Configure cross-domain authentication - Enable centralized sign-in (SSO) - Integrate with AWS WorkSpaces 13/11/2025 13/11/2025 AWS SSO Docs 4 - Practice directory troubleshooting - Monitor domain health and replication status - Configure backup and rollback processes 14/11/2025 14/11/2025 AWS Managed AD Best Practices 5 - Final weekly lab: + Deploy a functional Hybrid AD + Configure bidirectional trust + Test authentication \u0026amp; SSO + Document real use cases 15/11/2025 15/11/2025 AWS Enterprise Architecture Notes Week 10 Outcomes: Built a strong understanding of Hybrid Directory Infrastructure behavior and architecture. Successfully configured Domain Trust between AWS and local AD servers. Learned practical selection of Managed AD, AD Connector, or Simple AD per use case. Implemented centralized authentication \u0026amp; SSO for AWS applications. Gained ability to monitor, troubleshoot, and recover directory services when failures occur. Summary: By the end of Week 10, I was able to:\nDesign enterprise-use Hybrid AD architectures on AWS. Connect on-prem Active Directory with AWS Directory Service securely. Implement cross-domain authentication and SSO flows. Select the most suitable directory service for different business scenarios. Maintain, monitor, and resolve real directory issues in production-like environments. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Gain an in-depth understanding of High Availability (HA) in cloud architecture. Design and operate a multi-tier web application architecture. Learn and compare different Elastic Load Balancer types (ALB, NLB, CLB) and their use cases. Implement backup, replication, and disaster recovery (DR) strategies for production workloads. Weekly Task Breakdown: Day Task Start Done References 1 - Introduction to HA in cloud environments - Concepts: fault tolerance \u0026amp; resilience - Study Reliability Pillar in Well-Architected Framework 18/11/2025 18/11/2025 AWS Well-Architected Docs 2 - Deep dive into ELB - Differences between ALB, NLB, CLB + use cases - Create and test an Application Load Balancer 19/11/2025 19/11/2025 AWS ELB Service Guide 3 - Design 3-tier architecture Web → App → DB - Configure multi-AZ load balancing - Build demo multi-layered environment 20/11/2025 20/11/2025 AWS Multi-Tier Architecture Notes 4 - Implement backup \u0026amp; restore for RDS and EBS - Test cross-region replication - Build DR strategy and failover plan 21/11/2025 21/11/2025 AWS DR Whitepaper 5 - Full Lab Implementation: + Build HA Web App on AWS + ALB + Auto Scaling working together + Multi-AZ RDS + Read Replica + Execute DR failover testing 22/11/2025 22/11/2025 FCJ Hands-on Workshop Week 11 Outcomes: Solid understanding of High Availability \u0026amp; Fault Tolerance in practice. Successfully configured and tested multiple Elastic Load Balancer types. Completed a scalable 3-tier architecture with multi-AZ redundancy. Designed a working Disaster Recovery plan with safe backup strategies. Built an application capable of operating reliably even under failure scenarios. Summary: By the end of the week, I was able to:\nDesign HA-based infrastructure aligned with AWS best practices. Operate Application Load Balancer for smart traffic routing. Build multi-tier and multi-AZ architectures that scale automatically. Configure automated backups and implement DR mechanisms. Perform live failover simulations to ensure production readiness. "
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: This content is for reference only. Please do not reuse or submit it verbatim, including this disclaimer.\nWeek 12 Objectives: Connect with members of First Cloud Journey and get familiar with the working environment. Gain a foundational understanding of AWS services and how to operate them via the Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduced to FCJ members - Read and summarize internship regulations and working policies 08/11/2025 08/11/2025 3 - Studied the basic AWS service categories: + Compute + Storage + Networking + Database + Additional service families 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Created an AWS Free Tier account - Explored AWS Console and AWS CLI - Hands-on tasks: + Register AWS account + Install \u0026amp; configure AWS CLI + Execute basic CLI commands 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learned about EC2 fundamentals: + Instance families + AMI + EBS + Other components - Practiced SSH connection to EC2 - Explored Elastic IP usage 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on practice: + Launch an EC2 instance + SSH access + Attach an additional EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Developed a foundational understanding of AWS and the primary service categories:\nCompute Storage Networking Database and more Successfully registered and configured an AWS Free Tier account.\nBecame comfortable navigating the AWS Management Console — locating services, viewing resources, and executing operations through the web UI.\nInstalled and configured AWS CLI with required parameters such as:\nAccess Key Secret Key Default Region and additional environment settings Performed basic operations through AWS CLI including:\nChecking account credentials \u0026amp; configuration Listing global regions Viewing EC2 configurations Creating \u0026amp; managing key pairs Checking currently running services and other exploratory commands Demonstrated the ability to manage AWS resources using both the Console and CLI interchangeably.\n\u0026hellip;\n"
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thienluhoan.github.io/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]